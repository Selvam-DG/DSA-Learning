# Divide and Conquer Strategy

##  Concept

Divide and Conquer is a powerful algorithm design paradigm where a problem is broken down into smaller subproblems, solved individually, and then combined to solve the original problem.

This technique is used in many efficient algorithms such as **Merge Sort**, **Quick Sort**, **Binary Search**, and **Strassenâ€™s Matrix Multiplication**.

---

##  Strategy Breakdown

1. **Divide**  
   Break the problem into smaller subproblems of the same type.

2. **Conquer**  
   Solve the subproblems recursively. If the subproblem is small enough, solve it directly.

3. **Combine**  
   Combine the results of the subproblems to form the solution of the original problem.

---

##  Key Properties

- Often uses recursion.
- Reduces time complexity by solving smaller instances.
- Efficient in problems with overlapping substructure or optimal substructure.

---

##  Time Complexity Pattern

If a problem of size `n` is divided into `a` subproblems each of size `n/b` and combining takes `O(n^d)` time, then the time complexity is given by the **Master Theorem**:

T(n) = aT(n/b) + O(n^d)


---

##  Examples of Divide and Conquer

| Algorithm             | Divide Step             | Combine Step                  |
|----------------------|--------------------------|--------------------------------|
| Merge Sort           | Split array in halves   | Merge sorted halves            |
| Quick Sort           | Choose pivot, partition | No combine step needed         |
| Binary Search        | Pick middle element     | No combine step needed         |
| Matrix Multiplication| Divide matrices         | Combine matrix results         |

---

##  Real-World Analogy

Imagine organizing a large pile of papers:
- Divide it into smaller piles,
- Sort each small pile individually,
- Merge them together.

---

##  Recurrence Relations of Common Divide and Conquer Algorithms
Understanding recurrence relations helps in analyzing the time complexity of divide-and-conquer algorithms using techniques like the **Master Theorem**, **Recursion Tree**, or **Substitution Method**.

T(n) = T(n-1) + 1 => O(n)

T(n) = T(n-1) + n => O(nÂ²)

T(n) = T(n-1) + logn => O(nlogn)

T(n) = T(n-1) + nÂ² => O(nÂ³)

T(n) = T(n-k) + 1 => O(n) where k in(1,2,3,...n)

T(n) = 2T(n-1) + 1 => O(2^n)

T(n) = 3T(n-1) + 1 => O(3^n)


**Master Theorem for Decreasing function:**
- T(n) = aT(n-b) + f(n) , a>0, b>0 and f(n)=O(n^k) where k >= 0
- case1:
  
    if a < 1, then Time Complexity is O(n) = f(n)
- case 2:
  
    if a = 1, then Time Complexity O(n) = O(n * f(n))
-  case 3:
- 
    if a > 1 then Time Complexity is O(n) = O( f(n) * a^(n/b) )
    

| Algorithm              | Recurrence Relation            | Time Complexity     |
|------------------------|--------------------------------|---------------------|
| **Binary Search**      | T(n) = T(n/2) + O(1)           | O(log n)            |
| **Merge Sort**         | T(n) = 2T(n/2) + O(n)          | O(n log n)          |
| **Quick Sort (avg)**   | T(n) = 2T(n/2) + O(n)          | O(n log n)          |
| **Quick Sort (worst)** | T(n) = T(n-1) + O(n)           | O(nÂ²)               |
| **Strassen's Matrix Multiplication** | T(n) = 7T(n/2) + O(nÂ²) | O(n^2.81)           |
| **Karatsuba's Algorithm** | T(n) = 3T(n/2) + O(n)        | O(n^1.585)          |
| **Tower of Hanoi**     | T(n) = 2T(n-1) + O(1)          | O(2^n)              |
| **Counting Inversions**| T(n) = 2T(n/2) + O(n)          | O(n log n)          |

---



## ðŸ“˜ Master Theorem (Quick Reference)

To solve recurrences of the form:  
T(n) = aT(n/b) + O(n^d)

We compare `n^d` with `n^log_b(a)`:

- **Case 1**: If `d < log_b(a)` â†’ T(n) = Î˜(n^log_b(a))
- **Case 2**: If `d = log_b(a)` â†’ T(n) = Î˜(n^d Â· log n)
- **Case 3**: If `d > log_b(a)` â†’ T(n) = Î˜(n^d)

Use this to quickly analyze divide-and-conquer recurrences!

---


## References

- [Abdul Bari - Divide and Conquer (YouTube)](https://www.youtube.com/watch?v=2Rr2tW9zvRg&list=PLDN4rrl48XKpZkf03iYFl-O29szjTrs_O&index=18&ab_channel=AbdulBari)  






